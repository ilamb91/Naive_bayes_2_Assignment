{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68be6af-a0b3-432d-bbdf-d754e819ba0c",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3275647-3513-4ffd-a574-9d47a9b82d7b",
   "metadata": {},
   "source": [
    "A1\n",
    "\n",
    "To find the probability that an employee is a smoker given that they use the health insurance plan, you can use Bayes' theorem. \n",
    "\n",
    "Let's define the following events:\n",
    "- Event A: An employee uses the health insurance plan.\n",
    "- Event B: An employee is a smoker.\n",
    "\n",
    "You are given the following probabilities:\n",
    "- \\(P(A)\\), the probability that an employee uses the health insurance plan, is 70% or 0.7.\n",
    "- \\(P(B|A)\\), the probability that an employee is a smoker given that they use the health insurance plan, is 40% or 0.4.\n",
    "\n",
    "Now, you want to find \\(P(B|A)\\), the probability that an employee is a smoker given that they use the health insurance plan. You can use Bayes' theorem for this:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "Where:\n",
    "- \\(P(A|B)\\) is the probability that an employee uses the health insurance plan given that they are a smoker. This information is not explicitly given, so you may need more data to calculate it.\n",
    "- \\(P(B)\\) is the prior probability that an employee is a smoker, which is what you want to find.\n",
    "- \\(P(A)\\) is the probability that an employee uses the health insurance plan, which is given as 0.7.\n",
    "\n",
    "Without the probability \\(P(A|B)\\), you won't be able to calculate \\(P(B|A)\\) directly. You would need either additional data or assumptions about the relationship between using the health insurance plan and being a smoker to proceed further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe5a91-4f9a-4a62-9d24-83858b91e8b7",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a063-0513-4c13-b1cf-643d4837a62b",
   "metadata": {},
   "source": [
    "A2\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm used in machine learning and natural language processing. They are primarily used for text classification and have some key differences based on the type of data they are designed to handle:\n",
    "\n",
    "**Bernoulli Naive Bayes:**\n",
    "\n",
    "1. **Data Type**: Bernoulli Naive Bayes is suitable for binary data, where features are either present (1) or absent (0). It's often used in document classification tasks where the presence or absence of specific words or features in a document matters.\n",
    "\n",
    "2. **Feature Representation**: It assumes that the features are generated from a Bernoulli distribution, meaning it models whether a feature is \"on\" or \"off.\" In text classification, this often means modeling whether a word occurs in a document (1 for present, 0 for absent).\n",
    "\n",
    "3. **Feature Independence**: Like other Naive Bayes variants, Bernoulli Naive Bayes assumes that features are conditionally independent given the class label. It simplifies the modeling by assuming that the presence or absence of one feature does not affect the presence or absence of another feature, given the class.\n",
    "\n",
    "4. **Common Use Cases**: Bernoulli Naive Bayes is commonly used for tasks like spam detection, sentiment analysis, and document classification, where binary feature representations are used to represent the presence or absence of words or features.\n",
    "\n",
    "**Multinomial Naive Bayes:**\n",
    "\n",
    "1. **Data Type**: Multinomial Naive Bayes is designed for data with multiple discrete categories or counts, such as word frequencies in text data. It is especially suitable for text classification tasks.\n",
    "\n",
    "2. **Feature Representation**: It assumes that the features follow a multinomial distribution, which is useful for modeling the frequency of discrete items (e.g., word counts) in a document.\n",
    "\n",
    "3. **Feature Independence**: Like other Naive Bayes variants, Multinomial Naive Bayes assumes that features are conditionally independent given the class label. In text classification, this means assuming that the frequency of each word is independent of the frequency of other words, given the class label.\n",
    "\n",
    "4. **Common Use Cases**: Multinomial Naive Bayes is commonly used for tasks like document classification (e.g., categorizing news articles into topics), spam filtering, and text mining, where word frequencies or counts are used as features.\n",
    "\n",
    "In summary, the key difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are suited for. Bernoulli Naive Bayes is for binary data (presence or absence of features), while Multinomial Naive Bayes is for data with multiple discrete categories (such as word frequencies). Both are variations of the Naive Bayes algorithm and rely on the assumption of feature independence given the class label. The choice between them depends on the nature of your data and the specific problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8546de-8ee3-4102-b4b8-24a4525bce20",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0a47b-6bce-491a-af3c-3bc6150b2112",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "Bernoulli Naive Bayes, like other Naive Bayes variants, generally assumes that features are binary, representing the presence (1) or absence (0) of specific features or characteristics. When dealing with missing values, you need to decide how to handle them to fit this binary assumption. Here are some common approaches:\n",
    "\n",
    "1. **Imputation**: One approach is to impute (fill in) missing values with a default value that represents the absence of the feature. In the context of Bernoulli Naive Bayes, you might impute missing values with 0, indicating that the feature is absent. This approach assumes that missing values are equivalent to the absence of the feature.\n",
    "\n",
    "2. **Ignore Missing Values**: Another option is to simply ignore instances with missing values during training and classification. This approach can be reasonable if missing values are relatively rare and not systematically related to the class labels. In this case, you would treat missing values as if they don't provide any information and exclude instances with missing values from your analysis.\n",
    "\n",
    "3. **Impute with Probability**: Instead of using a fixed value like 0, you can estimate the probability of the feature being 1 based on the available data and impute missing values with this estimated probability. For example, you might impute missing values with the estimated probability of the feature being 1 within the class to which the instance belongs.\n",
    "\n",
    "4. **Use a Special Category**: You can create a special category or level for missing values, treating them as a separate feature value. This approach acknowledges that missing values are distinct from 0 and can carry information. However, it also increases the dimensionality of your feature space, potentially leading to increased computational complexity.\n",
    "\n",
    "The choice of how to handle missing values in Bernoulli Naive Bayes should be based on the specific characteristics of your data and the problem you are trying to solve. It's important to consider whether missing values are missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR), as this can influence your imputation strategy. Additionally, the impact of missing values on the performance of your classifier should be carefully evaluated through techniques like cross-validation.\n",
    "\n",
    "Keep in mind that Naive Bayes algorithms, including Bernoulli Naive Bayes, rely on strong assumptions about the independence of features given the class label. Depending on how you handle missing values, these assumptions may be affected, and the performance of the classifier may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a26c3b-54da-47d4-910e-7e9f9b8aad18",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0108b-efc4-4968-88af-326b6c127484",
   "metadata": {},
   "source": [
    "A4\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that is well-suited for data with continuous features that can be modeled using a Gaussian (normal) distribution. It is often used for binary and multi-class classification tasks.\n",
    "\n",
    "In multi-class classification, the goal is to classify instances into one of several possible classes or categories. Gaussian Naive Bayes can be adapted for multi-class problems using various techniques, including:\n",
    "\n",
    "1. **One-vs-Rest (OvR) or One-vs-All (OvA)**: In this approach, you train a separate binary Gaussian Naive Bayes classifier for each class, treating one class as the \"positive\" class and all other classes as the \"negative\" class. During prediction, each classifier assigns a probability or score for its corresponding class, and the class with the highest score is chosen as the final prediction.\n",
    "\n",
    "2. **Softmax Regression**: Gaussian Naive Bayes can be extended to multi-class classification by using a softmax function to compute class probabilities. This approach is similar to logistic regression, where the softmax function is applied to the output of the Gaussian Naive Bayes classifier to produce a probability distribution over all classes.\n",
    "\n",
    "3. **Custom Probability Aggregation**: You can also implement custom strategies to aggregate the class probabilities or scores produced by the Gaussian Naive Bayes classifier. For example, you might use weighted voting or other combination methods to make the final class prediction.\n",
    "\n",
    "Here's a high-level overview of how multi-class classification works with Gaussian Naive Bayes using the OvR strategy:\n",
    "\n",
    "1. Train a separate Gaussian Naive Bayes classifier for each class, where each classifier models the distribution of features for that class as a Gaussian.\n",
    "\n",
    "2. During prediction, obtain the probability (or score) that an instance belongs to each class using each of the trained classifiers.\n",
    "\n",
    "3. Assign the instance to the class with the highest probability or score among all the classifiers.\n",
    "\n",
    "Gaussian Naive Bayes is a useful choice for multi-class classification when you have continuous data and can assume that the features are normally distributed within each class. It's a simple and efficient algorithm for handling such scenarios, but it does make the strong assumption of feature independence within each class, which may or may not hold true in your specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78881ad2-d6a8-4c8a-a902-5cd5d5b7f573",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "\n",
    "- Data preparation: Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "- Implementation: Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "- Results: Report the following performance metrics for each classifier:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 score\n",
    "\n",
    "- Discussion: Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "- Conclusion: Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2c75af-e133-4f9e-a15b-506f18d30c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.2-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c785389d-cedd-43fe-b272-0d71721069ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 94, 'name': 'Spambase', 'repository_url': 'https://archive.ics.uci.edu/dataset/94/spambase', 'data_url': 'https://archive.ics.uci.edu/static/public/94/data.csv', 'abstract': 'Classifying Email as Spam or Non-Spam', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 4601, 'num_features': 57, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1999, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C53G6X', 'creators': ['Mark Hopkins', 'Erik Reeber', 'George Forman', 'Jaap Suermondt'], 'intro_paper': None, 'additional_info': {'summary': 'The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...\\n\\nThe classification task for this dataset is to determine whether a given email is spam or not.\\n\\t\\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam.  Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \\'george\\' and the area code \\'650\\' are indicators of non-spam.  These are useful when constructing a personalized spam filter.  One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\\n\\nFor background on spam: Cranor, Lorrie F., LaMacchia, Brian A.  Spam!, Communications of the ACM, 41(8):74-83, 1998.\\n\\nTypical performance is around ~7% misclassification error. False positives (marking good mail as spam) are very undesirable.If we insist on zero false positives in the training/testing set, 20-25% of the spam passed through the filter. See also Hewlett-Packard Internal-only Technical Report. External version forthcoming. ', 'purpose': None, 'funded_by': None, 'instances_represent': 'Emails', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The last column of \\'spambase.data\\' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.  The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.  For the statistical measures of each attribute, see the end of this file.  Here are the definitions of the attributes:\\r\\n\\r\\n48 continuous real [0,100] attributes of type word_freq_WORD \\r\\n= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\\r\\n\\r\\n6 continuous real [0,100] attributes of type char_freq_CHAR] \\r\\n= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\\r\\n\\r\\n1 continuous real [1,...] attribute of type capital_run_length_average \\r\\n= average length of uninterrupted sequences of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_longest \\r\\n= length of longest uninterrupted sequence of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_total \\r\\n= sum of length of uninterrupted sequences of capital letters \\r\\n= total number of capital letters in the e-mail\\r\\n\\r\\n1 nominal {0,1} class attribute of type spam\\r\\n= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \\r\\n', 'citation': None}}\n",
      "                          name     role        type demographic  \\\n",
      "0               word_freq_make  Feature  Continuous        None   \n",
      "1            word_freq_address  Feature  Continuous        None   \n",
      "2                word_freq_all  Feature  Continuous        None   \n",
      "3                 word_freq_3d  Feature  Continuous        None   \n",
      "4                word_freq_our  Feature  Continuous        None   \n",
      "5               word_freq_over  Feature  Continuous        None   \n",
      "6             word_freq_remove  Feature  Continuous        None   \n",
      "7           word_freq_internet  Feature  Continuous        None   \n",
      "8              word_freq_order  Feature  Continuous        None   \n",
      "9               word_freq_mail  Feature  Continuous        None   \n",
      "10           word_freq_receive  Feature  Continuous        None   \n",
      "11              word_freq_will  Feature  Continuous        None   \n",
      "12            word_freq_people  Feature  Continuous        None   \n",
      "13            word_freq_report  Feature  Continuous        None   \n",
      "14         word_freq_addresses  Feature  Continuous        None   \n",
      "15              word_freq_free  Feature  Continuous        None   \n",
      "16          word_freq_business  Feature  Continuous        None   \n",
      "17             word_freq_email  Feature  Continuous        None   \n",
      "18               word_freq_you  Feature  Continuous        None   \n",
      "19            word_freq_credit  Feature  Continuous        None   \n",
      "20              word_freq_your  Feature  Continuous        None   \n",
      "21              word_freq_font  Feature  Continuous        None   \n",
      "22               word_freq_000  Feature  Continuous        None   \n",
      "23             word_freq_money  Feature  Continuous        None   \n",
      "24                word_freq_hp  Feature  Continuous        None   \n",
      "25               word_freq_hpl  Feature  Continuous        None   \n",
      "26            word_freq_george  Feature  Continuous        None   \n",
      "27               word_freq_650  Feature  Continuous        None   \n",
      "28               word_freq_lab  Feature  Continuous        None   \n",
      "29              word_freq_labs  Feature  Continuous        None   \n",
      "30            word_freq_telnet  Feature  Continuous        None   \n",
      "31               word_freq_857  Feature  Continuous        None   \n",
      "32              word_freq_data  Feature  Continuous        None   \n",
      "33               word_freq_415  Feature  Continuous        None   \n",
      "34                word_freq_85  Feature  Continuous        None   \n",
      "35        word_freq_technology  Feature  Continuous        None   \n",
      "36              word_freq_1999  Feature  Continuous        None   \n",
      "37             word_freq_parts  Feature  Continuous        None   \n",
      "38                word_freq_pm  Feature  Continuous        None   \n",
      "39            word_freq_direct  Feature  Continuous        None   \n",
      "40                word_freq_cs  Feature  Continuous        None   \n",
      "41           word_freq_meeting  Feature  Continuous        None   \n",
      "42          word_freq_original  Feature  Continuous        None   \n",
      "43           word_freq_project  Feature  Continuous        None   \n",
      "44                word_freq_re  Feature  Continuous        None   \n",
      "45               word_freq_edu  Feature  Continuous        None   \n",
      "46             word_freq_table  Feature  Continuous        None   \n",
      "47        word_freq_conference  Feature  Continuous        None   \n",
      "48                 char_freq_;  Feature  Continuous        None   \n",
      "49                 char_freq_(  Feature  Continuous        None   \n",
      "50                 char_freq_[  Feature  Continuous        None   \n",
      "51                 char_freq_!  Feature  Continuous        None   \n",
      "52                 char_freq_$  Feature  Continuous        None   \n",
      "53                 char_freq_#  Feature  Continuous        None   \n",
      "54  capital_run_length_average  Feature  Continuous        None   \n",
      "55  capital_run_length_longest  Feature  Continuous        None   \n",
      "56    capital_run_length_total  Feature  Continuous        None   \n",
      "57                       Class   Target      Binary        None   \n",
      "\n",
      "                 description units missing_values  \n",
      "0                       None  None             no  \n",
      "1                       None  None             no  \n",
      "2                       None  None             no  \n",
      "3                       None  None             no  \n",
      "4                       None  None             no  \n",
      "5                       None  None             no  \n",
      "6                       None  None             no  \n",
      "7                       None  None             no  \n",
      "8                       None  None             no  \n",
      "9                       None  None             no  \n",
      "10                      None  None             no  \n",
      "11                      None  None             no  \n",
      "12                      None  None             no  \n",
      "13                      None  None             no  \n",
      "14                      None  None             no  \n",
      "15                      None  None             no  \n",
      "16                      None  None             no  \n",
      "17                      None  None             no  \n",
      "18                      None  None             no  \n",
      "19                      None  None             no  \n",
      "20                      None  None             no  \n",
      "21                      None  None             no  \n",
      "22                      None  None             no  \n",
      "23                      None  None             no  \n",
      "24                      None  None             no  \n",
      "25                      None  None             no  \n",
      "26                      None  None             no  \n",
      "27                      None  None             no  \n",
      "28                      None  None             no  \n",
      "29                      None  None             no  \n",
      "30                      None  None             no  \n",
      "31                      None  None             no  \n",
      "32                      None  None             no  \n",
      "33                      None  None             no  \n",
      "34                      None  None             no  \n",
      "35                      None  None             no  \n",
      "36                      None  None             no  \n",
      "37                      None  None             no  \n",
      "38                      None  None             no  \n",
      "39                      None  None             no  \n",
      "40                      None  None             no  \n",
      "41                      None  None             no  \n",
      "42                      None  None             no  \n",
      "43                      None  None             no  \n",
      "44                      None  None             no  \n",
      "45                      None  None             no  \n",
      "46                      None  None             no  \n",
      "47                      None  None             no  \n",
      "48                      None  None             no  \n",
      "49                      None  None             no  \n",
      "50                      None  None             no  \n",
      "51                      None  None             no  \n",
      "52                      None  None             no  \n",
      "53                      None  None             no  \n",
      "54                      None  None             no  \n",
      "55                      None  None             no  \n",
      "56                      None  None             no  \n",
      "57  spam (1) or not spam (0)  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(spambase.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(spambase.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fb2238-c952-4055-8a08-41f89a914ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'ids': None,\n",
       "  'features':       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "  0               0.00               0.64           0.64           0.0   \n",
       "  1               0.21               0.28           0.50           0.0   \n",
       "  2               0.06               0.00           0.71           0.0   \n",
       "  3               0.00               0.00           0.00           0.0   \n",
       "  4               0.00               0.00           0.00           0.0   \n",
       "  ...              ...                ...            ...           ...   \n",
       "  4596            0.31               0.00           0.62           0.0   \n",
       "  4597            0.00               0.00           0.00           0.0   \n",
       "  4598            0.30               0.00           0.30           0.0   \n",
       "  4599            0.96               0.00           0.00           0.0   \n",
       "  4600            0.00               0.00           0.65           0.0   \n",
       "  \n",
       "        word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "  0              0.32            0.00              0.00                0.00   \n",
       "  1              0.14            0.28              0.21                0.07   \n",
       "  2              1.23            0.19              0.19                0.12   \n",
       "  3              0.63            0.00              0.31                0.63   \n",
       "  4              0.63            0.00              0.31                0.63   \n",
       "  ...             ...             ...               ...                 ...   \n",
       "  4596           0.00            0.31              0.00                0.00   \n",
       "  4597           0.00            0.00              0.00                0.00   \n",
       "  4598           0.00            0.00              0.00                0.00   \n",
       "  4599           0.32            0.00              0.00                0.00   \n",
       "  4600           0.00            0.00              0.00                0.00   \n",
       "  \n",
       "        word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "  0                0.00            0.00  ...                   0.0        0.000   \n",
       "  1                0.00            0.94  ...                   0.0        0.000   \n",
       "  2                0.64            0.25  ...                   0.0        0.010   \n",
       "  3                0.31            0.63  ...                   0.0        0.000   \n",
       "  4                0.31            0.63  ...                   0.0        0.000   \n",
       "  ...               ...             ...  ...                   ...          ...   \n",
       "  4596             0.00            0.00  ...                   0.0        0.000   \n",
       "  4597             0.00            0.00  ...                   0.0        0.000   \n",
       "  4598             0.00            0.00  ...                   0.0        0.102   \n",
       "  4599             0.00            0.00  ...                   0.0        0.000   \n",
       "  4600             0.00            0.00  ...                   0.0        0.000   \n",
       "  \n",
       "        char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "  0           0.000          0.0        0.778        0.000        0.000   \n",
       "  1           0.132          0.0        0.372        0.180        0.048   \n",
       "  2           0.143          0.0        0.276        0.184        0.010   \n",
       "  3           0.137          0.0        0.137        0.000        0.000   \n",
       "  4           0.135          0.0        0.135        0.000        0.000   \n",
       "  ...           ...          ...          ...          ...          ...   \n",
       "  4596        0.232          0.0        0.000        0.000        0.000   \n",
       "  4597        0.000          0.0        0.353        0.000        0.000   \n",
       "  4598        0.718          0.0        0.000        0.000        0.000   \n",
       "  4599        0.057          0.0        0.000        0.000        0.000   \n",
       "  4600        0.000          0.0        0.125        0.000        0.000   \n",
       "  \n",
       "        capital_run_length_average  capital_run_length_longest  \\\n",
       "  0                          3.756                          61   \n",
       "  1                          5.114                         101   \n",
       "  2                          9.821                         485   \n",
       "  3                          3.537                          40   \n",
       "  4                          3.537                          40   \n",
       "  ...                          ...                         ...   \n",
       "  4596                       1.142                           3   \n",
       "  4597                       1.555                           4   \n",
       "  4598                       1.404                           6   \n",
       "  4599                       1.147                           5   \n",
       "  4600                       1.250                           5   \n",
       "  \n",
       "        capital_run_length_total  \n",
       "  0                          278  \n",
       "  1                         1028  \n",
       "  2                         2259  \n",
       "  3                          191  \n",
       "  4                          191  \n",
       "  ...                        ...  \n",
       "  4596                        88  \n",
       "  4597                        14  \n",
       "  4598                       118  \n",
       "  4599                        78  \n",
       "  4600                        40  \n",
       "  \n",
       "  [4601 rows x 57 columns],\n",
       "  'targets':       Class\n",
       "  0         1\n",
       "  1         1\n",
       "  2         1\n",
       "  3         1\n",
       "  4         1\n",
       "  ...     ...\n",
       "  4596      0\n",
       "  4597      0\n",
       "  4598      0\n",
       "  4599      0\n",
       "  4600      0\n",
       "  \n",
       "  [4601 rows x 1 columns],\n",
       "  'original':       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "  0               0.00               0.64           0.64           0.0   \n",
       "  1               0.21               0.28           0.50           0.0   \n",
       "  2               0.06               0.00           0.71           0.0   \n",
       "  3               0.00               0.00           0.00           0.0   \n",
       "  4               0.00               0.00           0.00           0.0   \n",
       "  ...              ...                ...            ...           ...   \n",
       "  4596            0.31               0.00           0.62           0.0   \n",
       "  4597            0.00               0.00           0.00           0.0   \n",
       "  4598            0.30               0.00           0.30           0.0   \n",
       "  4599            0.96               0.00           0.00           0.0   \n",
       "  4600            0.00               0.00           0.65           0.0   \n",
       "  \n",
       "        word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "  0              0.32            0.00              0.00                0.00   \n",
       "  1              0.14            0.28              0.21                0.07   \n",
       "  2              1.23            0.19              0.19                0.12   \n",
       "  3              0.63            0.00              0.31                0.63   \n",
       "  4              0.63            0.00              0.31                0.63   \n",
       "  ...             ...             ...               ...                 ...   \n",
       "  4596           0.00            0.31              0.00                0.00   \n",
       "  4597           0.00            0.00              0.00                0.00   \n",
       "  4598           0.00            0.00              0.00                0.00   \n",
       "  4599           0.32            0.00              0.00                0.00   \n",
       "  4600           0.00            0.00              0.00                0.00   \n",
       "  \n",
       "        word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "  0                0.00            0.00  ...        0.000        0.000   \n",
       "  1                0.00            0.94  ...        0.000        0.132   \n",
       "  2                0.64            0.25  ...        0.010        0.143   \n",
       "  3                0.31            0.63  ...        0.000        0.137   \n",
       "  4                0.31            0.63  ...        0.000        0.135   \n",
       "  ...               ...             ...  ...          ...          ...   \n",
       "  4596             0.00            0.00  ...        0.000        0.232   \n",
       "  4597             0.00            0.00  ...        0.000        0.000   \n",
       "  4598             0.00            0.00  ...        0.102        0.718   \n",
       "  4599             0.00            0.00  ...        0.000        0.057   \n",
       "  4600             0.00            0.00  ...        0.000        0.000   \n",
       "  \n",
       "        char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "  0             0.0        0.778        0.000        0.000   \n",
       "  1             0.0        0.372        0.180        0.048   \n",
       "  2             0.0        0.276        0.184        0.010   \n",
       "  3             0.0        0.137        0.000        0.000   \n",
       "  4             0.0        0.135        0.000        0.000   \n",
       "  ...           ...          ...          ...          ...   \n",
       "  4596          0.0        0.000        0.000        0.000   \n",
       "  4597          0.0        0.353        0.000        0.000   \n",
       "  4598          0.0        0.000        0.000        0.000   \n",
       "  4599          0.0        0.000        0.000        0.000   \n",
       "  4600          0.0        0.125        0.000        0.000   \n",
       "  \n",
       "        capital_run_length_average  capital_run_length_longest  \\\n",
       "  0                          3.756                          61   \n",
       "  1                          5.114                         101   \n",
       "  2                          9.821                         485   \n",
       "  3                          3.537                          40   \n",
       "  4                          3.537                          40   \n",
       "  ...                          ...                         ...   \n",
       "  4596                       1.142                           3   \n",
       "  4597                       1.555                           4   \n",
       "  4598                       1.404                           6   \n",
       "  4599                       1.147                           5   \n",
       "  4600                       1.250                           5   \n",
       "  \n",
       "        capital_run_length_total  Class  \n",
       "  0                          278      1  \n",
       "  1                         1028      1  \n",
       "  2                         2259      1  \n",
       "  3                          191      1  \n",
       "  4                          191      1  \n",
       "  ...                        ...    ...  \n",
       "  4596                        88      0  \n",
       "  4597                        14      0  \n",
       "  4598                       118      0  \n",
       "  4599                        78      0  \n",
       "  4600                        40      0  \n",
       "  \n",
       "  [4601 rows x 58 columns],\n",
       "  'headers': Index(['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
       "         'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
       "         'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
       "         'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
       "         'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
       "         'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
       "         'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
       "         'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
       "         'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
       "         'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
       "         'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
       "         'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
       "         'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
       "         'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
       "         'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
       "         'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
       "         'capital_run_length_longest', 'capital_run_length_total', 'Class'],\n",
       "        dtype='object')},\n",
       " 'metadata': {'uci_id': 94,\n",
       "  'name': 'Spambase',\n",
       "  'repository_url': 'https://archive.ics.uci.edu/dataset/94/spambase',\n",
       "  'data_url': 'https://archive.ics.uci.edu/static/public/94/data.csv',\n",
       "  'abstract': 'Classifying Email as Spam or Non-Spam',\n",
       "  'area': 'Computer Science',\n",
       "  'tasks': ['Classification'],\n",
       "  'characteristics': ['Multivariate'],\n",
       "  'num_instances': 4601,\n",
       "  'num_features': 57,\n",
       "  'feature_types': ['Integer', 'Real'],\n",
       "  'demographics': [],\n",
       "  'target_col': ['Class'],\n",
       "  'index_col': None,\n",
       "  'has_missing_values': 'no',\n",
       "  'missing_values_symbol': None,\n",
       "  'year_of_dataset_creation': 1999,\n",
       "  'last_updated': 'Mon Aug 28 2023',\n",
       "  'dataset_doi': '10.24432/C53G6X',\n",
       "  'creators': ['Mark Hopkins',\n",
       "   'Erik Reeber',\n",
       "   'George Forman',\n",
       "   'Jaap Suermondt'],\n",
       "  'intro_paper': None,\n",
       "  'additional_info': {'summary': 'The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...\\n\\nThe classification task for this dataset is to determine whether a given email is spam or not.\\n\\t\\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam.  Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \\'george\\' and the area code \\'650\\' are indicators of non-spam.  These are useful when constructing a personalized spam filter.  One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\\n\\nFor background on spam: Cranor, Lorrie F., LaMacchia, Brian A.  Spam!, Communications of the ACM, 41(8):74-83, 1998.\\n\\nTypical performance is around ~7% misclassification error. False positives (marking good mail as spam) are very undesirable.If we insist on zero false positives in the training/testing set, 20-25% of the spam passed through the filter. See also Hewlett-Packard Internal-only Technical Report. External version forthcoming. ',\n",
       "   'purpose': None,\n",
       "   'funded_by': None,\n",
       "   'instances_represent': 'Emails',\n",
       "   'recommended_data_splits': None,\n",
       "   'sensitive_data': None,\n",
       "   'preprocessing_description': None,\n",
       "   'variable_info': 'The last column of \\'spambase.data\\' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.  The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.  For the statistical measures of each attribute, see the end of this file.  Here are the definitions of the attributes:\\r\\n\\r\\n48 continuous real [0,100] attributes of type word_freq_WORD \\r\\n= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\\r\\n\\r\\n6 continuous real [0,100] attributes of type char_freq_CHAR] \\r\\n= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\\r\\n\\r\\n1 continuous real [1,...] attribute of type capital_run_length_average \\r\\n= average length of uninterrupted sequences of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_longest \\r\\n= length of longest uninterrupted sequence of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_total \\r\\n= sum of length of uninterrupted sequences of capital letters \\r\\n= total number of capital letters in the e-mail\\r\\n\\r\\n1 nominal {0,1} class attribute of type spam\\r\\n= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \\r\\n',\n",
       "   'citation': None}},\n",
       " 'variables':                           name     role        type demographic  \\\n",
       " 0               word_freq_make  Feature  Continuous        None   \n",
       " 1            word_freq_address  Feature  Continuous        None   \n",
       " 2                word_freq_all  Feature  Continuous        None   \n",
       " 3                 word_freq_3d  Feature  Continuous        None   \n",
       " 4                word_freq_our  Feature  Continuous        None   \n",
       " 5               word_freq_over  Feature  Continuous        None   \n",
       " 6             word_freq_remove  Feature  Continuous        None   \n",
       " 7           word_freq_internet  Feature  Continuous        None   \n",
       " 8              word_freq_order  Feature  Continuous        None   \n",
       " 9               word_freq_mail  Feature  Continuous        None   \n",
       " 10           word_freq_receive  Feature  Continuous        None   \n",
       " 11              word_freq_will  Feature  Continuous        None   \n",
       " 12            word_freq_people  Feature  Continuous        None   \n",
       " 13            word_freq_report  Feature  Continuous        None   \n",
       " 14         word_freq_addresses  Feature  Continuous        None   \n",
       " 15              word_freq_free  Feature  Continuous        None   \n",
       " 16          word_freq_business  Feature  Continuous        None   \n",
       " 17             word_freq_email  Feature  Continuous        None   \n",
       " 18               word_freq_you  Feature  Continuous        None   \n",
       " 19            word_freq_credit  Feature  Continuous        None   \n",
       " 20              word_freq_your  Feature  Continuous        None   \n",
       " 21              word_freq_font  Feature  Continuous        None   \n",
       " 22               word_freq_000  Feature  Continuous        None   \n",
       " 23             word_freq_money  Feature  Continuous        None   \n",
       " 24                word_freq_hp  Feature  Continuous        None   \n",
       " 25               word_freq_hpl  Feature  Continuous        None   \n",
       " 26            word_freq_george  Feature  Continuous        None   \n",
       " 27               word_freq_650  Feature  Continuous        None   \n",
       " 28               word_freq_lab  Feature  Continuous        None   \n",
       " 29              word_freq_labs  Feature  Continuous        None   \n",
       " 30            word_freq_telnet  Feature  Continuous        None   \n",
       " 31               word_freq_857  Feature  Continuous        None   \n",
       " 32              word_freq_data  Feature  Continuous        None   \n",
       " 33               word_freq_415  Feature  Continuous        None   \n",
       " 34                word_freq_85  Feature  Continuous        None   \n",
       " 35        word_freq_technology  Feature  Continuous        None   \n",
       " 36              word_freq_1999  Feature  Continuous        None   \n",
       " 37             word_freq_parts  Feature  Continuous        None   \n",
       " 38                word_freq_pm  Feature  Continuous        None   \n",
       " 39            word_freq_direct  Feature  Continuous        None   \n",
       " 40                word_freq_cs  Feature  Continuous        None   \n",
       " 41           word_freq_meeting  Feature  Continuous        None   \n",
       " 42          word_freq_original  Feature  Continuous        None   \n",
       " 43           word_freq_project  Feature  Continuous        None   \n",
       " 44                word_freq_re  Feature  Continuous        None   \n",
       " 45               word_freq_edu  Feature  Continuous        None   \n",
       " 46             word_freq_table  Feature  Continuous        None   \n",
       " 47        word_freq_conference  Feature  Continuous        None   \n",
       " 48                 char_freq_;  Feature  Continuous        None   \n",
       " 49                 char_freq_(  Feature  Continuous        None   \n",
       " 50                 char_freq_[  Feature  Continuous        None   \n",
       " 51                 char_freq_!  Feature  Continuous        None   \n",
       " 52                 char_freq_$  Feature  Continuous        None   \n",
       " 53                 char_freq_#  Feature  Continuous        None   \n",
       " 54  capital_run_length_average  Feature  Continuous        None   \n",
       " 55  capital_run_length_longest  Feature  Continuous        None   \n",
       " 56    capital_run_length_total  Feature  Continuous        None   \n",
       " 57                       Class   Target      Binary        None   \n",
       " \n",
       "                  description units missing_values  \n",
       " 0                       None  None             no  \n",
       " 1                       None  None             no  \n",
       " 2                       None  None             no  \n",
       " 3                       None  None             no  \n",
       " 4                       None  None             no  \n",
       " 5                       None  None             no  \n",
       " 6                       None  None             no  \n",
       " 7                       None  None             no  \n",
       " 8                       None  None             no  \n",
       " 9                       None  None             no  \n",
       " 10                      None  None             no  \n",
       " 11                      None  None             no  \n",
       " 12                      None  None             no  \n",
       " 13                      None  None             no  \n",
       " 14                      None  None             no  \n",
       " 15                      None  None             no  \n",
       " 16                      None  None             no  \n",
       " 17                      None  None             no  \n",
       " 18                      None  None             no  \n",
       " 19                      None  None             no  \n",
       " 20                      None  None             no  \n",
       " 21                      None  None             no  \n",
       " 22                      None  None             no  \n",
       " 23                      None  None             no  \n",
       " 24                      None  None             no  \n",
       " 25                      None  None             no  \n",
       " 26                      None  None             no  \n",
       " 27                      None  None             no  \n",
       " 28                      None  None             no  \n",
       " 29                      None  None             no  \n",
       " 30                      None  None             no  \n",
       " 31                      None  None             no  \n",
       " 32                      None  None             no  \n",
       " 33                      None  None             no  \n",
       " 34                      None  None             no  \n",
       " 35                      None  None             no  \n",
       " 36                      None  None             no  \n",
       " 37                      None  None             no  \n",
       " 38                      None  None             no  \n",
       " 39                      None  None             no  \n",
       " 40                      None  None             no  \n",
       " 41                      None  None             no  \n",
       " 42                      None  None             no  \n",
       " 43                      None  None             no  \n",
       " 44                      None  None             no  \n",
       " 45                      None  None             no  \n",
       " 46                      None  None             no  \n",
       " 47                      None  None             no  \n",
       " 48                      None  None             no  \n",
       " 49                      None  None             no  \n",
       " 50                      None  None             no  \n",
       " 51                      None  None             no  \n",
       " 52                      None  None             no  \n",
       " 53                      None  None             no  \n",
       " 54                      None  None             no  \n",
       " 55                      None  None             no  \n",
       " 56                      None  None             no  \n",
       " 57  spam (1) or not spam (0)  None             no  }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59eb8e80-5398-4b48-8276-06469ba8b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'spambase.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96b8308-2761-476e-9a07-5cccad6d5602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4600 rows  58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...   0.41  \\\n",
       "0     0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "1     0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "2     0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "3     0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.000   \n",
       "...    ...   ...     ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4595  0.31  0.00    0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4596  0.00  0.00    0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.30  0.00    0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4598  0.96  0.00    0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4599  0.00  0.00    0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "       0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0     0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1     0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2     0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3     0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4     0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "...     ...   ...    ...    ...    ...    ...  ...   ... ..  \n",
       "4595  0.232   0.0  0.000  0.000  0.000  1.142    3    88  0  \n",
       "4596  0.000   0.0  0.353  0.000  0.000  1.555    4    14  0  \n",
       "4597  0.718   0.0  0.000  0.000  0.000  1.404    6   118  0  \n",
       "4598  0.057   0.0  0.000  0.000  0.000  1.147    5    78  0  \n",
       "4599  0.000   0.0  0.125  0.000  0.000  1.250    5    40  0  \n",
       "\n",
       "[4600 rows x 58 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "302a4a0a-1d1c-4ede-b98f-d1c2b7c97e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes - Average Accuracy: 0.8839380364047911\n",
      "Multinomial Naive Bayes - Average Accuracy: 0.7863496180326323\n",
      "Gaussian Naive Bayes - Average Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "data = pd.read_csv(\"spambase.csv\", header=None)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Initialize the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate each classifier\n",
    "scores_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Report the average accuracy for each classifier\n",
    "print(\"Bernoulli Naive Bayes - Average Accuracy:\", scores_bernoulli.mean())\n",
    "print(\"Multinomial Naive Bayes - Average Accuracy:\", scores_multinomial.mean())\n",
    "print(\"Gaussian Naive Bayes - Average Accuracy:\", scores_gaussian.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c5ed6c-01f0-41fe-831b-c8585f233863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Bernoulli NB\n",
      "Accuracy: 0.8839130434782609\n",
      "Precision: 0.886914139754535\n",
      "Recall: 0.8151235504826666\n",
      "F1 Score: 0.8480714616697421\n",
      "\n",
      "Classifier: Multinomial NB\n",
      "Accuracy: 0.786086956521739\n",
      "Precision: 0.7390291264847734\n",
      "Recall: 0.7207971586424625\n",
      "F1 Score: 0.7277511309974372\n",
      "\n",
      "Classifier: Gaussian NB\n",
      "Accuracy: 0.8217391304347826\n",
      "Precision: 0.7102746648832371\n",
      "Recall: 0.9569394693704085\n",
      "F1 Score: 0.8129997873786424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "file_path = 'spambase.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop('1', axis=1)  \n",
    "y = data['1']\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "classifier_names = ['Bernoulli NB', 'Multinomial NB', 'Gaussian NB']\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier\n",
    "for classifier, name in zip(classifiers, classifier_names):\n",
    "    print(f\"Classifier: {name}\")\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score using cross-validation\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision')\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall')\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    # Calculate and print the average scores\n",
    "    print(f\"Accuracy: {accuracy.mean()}\")\n",
    "    print(f\"Precision: {precision.mean()}\")\n",
    "    print(f\"Recall: {recall.mean()}\")\n",
    "    print(f\"F1 Score: {f1.mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab700b0d-0291-4326-80a9-0f2cb82f43d7",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "Analyze the results and discuss which variant of Naive Bayes performed the best in terms of the specified metrics.\n",
    "Consider why one variant might outperform the others. For example, the choice between Bernoulli, Multinomial, or Gaussian Naive Bayes depends on the nature of the data. Discuss how the data's characteristics influenced the performance.\n",
    "Discuss any limitations or challenges you encountered during the analysis, such as handling missing values or choosing appropriate preprocessing steps.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarize your findings and provide insights into the performance of the different Naive Bayes variants on the given dataset.\n",
    "Offer suggestions for future work, such as exploring more advanced machine learning algorithms, feature engineering techniques, or fine-tuning hyperparameters.\n",
    "\n",
    "Remember that the actual implementation of this project involves coding in Python and using scikit-learn extensively. Each step requires careful consideration and coding, and you may need to refer to scikit-learn's documentation for details on how to use the classifiers and evaluation functions effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c70ae-f344-4975-a837-8bb2432d450a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
